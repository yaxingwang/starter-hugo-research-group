---
abstract: Conditional GANs have matured in recent years and are able to generate
  high-quality realistic images. However, the computational resources and the
  training data required for the training of high-quality GANs are enormous, and
  the study of transfer learning of these models is therefore an urgent topic.
  In this paper, we explore the transfer from high-quality pre-trained
  unconditional GANs to conditional GANs. To this end, we propose
  hypernetwork-based adaptive weight modulation. In addition, we introduce a
  self-initialization procedure that does not require any real data to
  initialize the hypernetwork parameters. To further improve the sample
  efficiency of the knowledge transfer, we propose to use a self-supervised
  (contrastive) loss to improve the GAN discriminator. In extensive experiments,
  we validate the efficiency of the hypernetworks, self-initialization and
  contrastive loss for knowledge transfer on several standard benchmarks.
view: citation
publication_types:
  - "1"
authors:
  - HÃ©ctor Laria
  - Yaxing Wang
  - Joost van de Weijer
  - Bogdan Raducanu
publication: In *Arxiv libary*
publication_short: In arxiv
summary: We explore first the setting from unconditional GANs to conditional GANs.
banner:
  caption: ""
  image: ""
title: "Hyper-GAN: Transferring Unconditional to Conditional GANs with HyperNetworks"
featured: true
tags:
  - Source Themes
projects:
  - Published project
image:
  filename: featured.png
date: 2022-03-20T04:04:32.188Z
---
{{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}



Code will be available.