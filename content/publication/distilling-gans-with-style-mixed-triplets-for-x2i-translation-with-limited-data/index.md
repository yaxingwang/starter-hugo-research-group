---
title: "Distilling GANs with Style-Mixed Triplets for X2I Translation with
  Limited Data "
abstract: "Conditional image synthesis is an integral part of many X2I
  translation systems, including image-to-image, text-to-image and
  audio-to-image translation systems. Training these large systems generally
  requires huge amounts of training data. Therefore, we investigate knowledge
  distillation to transfer knowledge from a high-quality unconditioned
  generative model (e.g., StyleGAN) to a conditioned synthetic image generation
  modules in a variety of systems. To initialize the conditional and reference
  branch (from a unconditional GAN)  we exploit the style mixing characteristics
  of high-quality GANs to generate an infinite supply of style-mixed triplets to
  perform the knowledge distillation. Extensive experimental results in a number
  of image generation tasks (i.e., image-to-image, semantic
  segmentation-to-image, text-to-image and audio-to-image) demonstrate
  qualitatively and quantitatively that our method successfully transfers
  knowledge to the synthetic image generation modules, resulting in more
  realistic images than previous methods as confirmed by a significant drop in
  the FID. "
slides: null
url_pdf: https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf
publication_types:
  - "1"
authors:
  - Yaxing Wang
  - Joost van de weijer
  - Lu Yu
  - Shangling Jui
publication: ICLR2022
featured: true
tags:
  - Source Themes
projects:
  - internal-project
summary: ""
url_dataset: "#"
url_project: ""
publication_short: ICLR2022
url_source: "#"
url_video: "#"
date: 2022-06-13T06:39:15.939Z
url_slides: ""
links:
  - name: Custom Link
    url: http://example.org
image:
  caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)"
  focal_point: ""
  preview_only: false
  filename: featured.png
publishDate: 2017-01-01T00:00:00.000Z
url_poster: "#"
url_code: https://github.com/Albert0147/SFDA_neighbors
doi: ""
---
ICLR2022.Code: <https://github.com/yaxingwang/KDIT>